# WK AI Service - FastAPI + Google Gemini

ServiÃ§o de inteligÃªncia artificial para anÃ¡lise de oportunidades de vendas e suporte via chatbot.

## ğŸš€ Funcionalidades

- **AnÃ¡lise de Risco de Oportunidades** - PontuaÃ§Ã£o automÃ¡tica usando IA
- **Chat com Assistente** - Responde perguntas sobre vendas e estratÃ©gia
- **Cache Inteligente** - Evita chamadas repetidas ao Gemini
- **CORS Habilitado** - Funciona com frontends em diferentes domÃ­nios
- **Logging Detalhado** - Rastreia todas as operaÃ§Ãµes

## ğŸ“‹ Requisitos

- Python 3.9+
- FastAPI
- Google Generative AI API (Gemini)
- Redis (opcional, para cache distribuÃ­do)

## âš™ï¸ InstalaÃ§Ã£o

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt

# 2. Configurar variÃ¡veis de ambiente
export GEMINI_API_KEY="sua_chave_aqui"

# 3. Iniciar o servidor
python main.py

# Ou com auto-reload para desenvolvimento:
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ”‘ ConfiguraÃ§Ã£o

### Google Gemini API Key

Para usar a IA de verdade:

1. Acesse https://makersuite.google.com/app/apikeys
2. Crie uma chave de API
3. Configure como variÃ¡vel de ambiente:

```bash
export GEMINI_API_KEY="AIzaSyD..."
```

Ou adicione ao `.env`:

```env
GEMINI_API_KEY=AIzaSyD...
```

## ğŸ“¡ Endpoints

### GET `/health`
Status do serviÃ§o
```bash
curl http://localhost:8000/health
```

### POST `/analyze`
Analisa uma oportunidade e retorna risco
```bash
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Projeto ERP Cloud",
    "value": 250000,
    "probability": 65,
    "status": "proposal",
    "customer_name": "TechCorp",
    "sector": "Tecnologia"
  }'
```

**Response:**
```json
{
  "risk_score": 35,
  "risk_label": "baixo",
  "next_action": "Apresentar proposta tÃ©cnica",
  "recommendation": "Agendar reuniÃ£o com CTO para validar arquitetura",
  "summary": "Oportunidade de alto valor com boa probabilidade.",
  "model": "gemini-pro",
  "cached": false
}
```

### POST `/api/v1/chat`
Chat com assistente de IA
```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Como aumentar taxa de conversÃ£o?",
    "context": {
      "user_id": "user-123"
    }
  }'
```

**Response:**
```json
{
  "answer": "A taxa de conversÃ£o pode ser aumentada atravÃ©s de...",
  "model": "gemini-pro",
  "source": "ai_service"
}
```

## ğŸ§ª Testes

```bash
# Executar teste automatizado (requer serviÃ§o rodando)
python test_api.py

# Ou individual com curl:
curl http://localhost:8000/analyze -X POST -d '{"title":"Test","value":100}' -H "Content-Type: application/json"
```

## ğŸ“Š Estrutura de Risk Score

| Score | Label | Cor | AÃ§Ã£o |
|-------|-------|-----|------|
| 0-33 | Baixo | ğŸŸ¢ Verde | Prosseguir normalmente |
| 34-66 | MÃ©dio | ğŸŸ¡ Amarelo | Acompanhar atentamente |
| 67-100 | Alto | ğŸ”´ Vermelho | Ativar plano de recuperaÃ§Ã£o |

## ğŸ”„ Fluxo de IntegraÃ§Ã£o com Laravel

1. **Frontend Vue/Angular** â†’ POST `/api/opportunities/{id}/ai-analysis`
2. **Laravel Controller** â†’ Valida dados
3. **Guzzle HTTP Client** â†’ Chama `http://wk-ai-service:8000/analyze`
4. **FastAPI** â†’ Processa com Gemini
5. **Response** â†’ Armazenada no DB
6. **Frontend** â†’ Exibe score e recomendaÃ§Ãµes

## ğŸ› ï¸ Desenvolvimento

```bash
# Modo desenvolvimento com auto-reload
uvicorn main:app --reload

# Estrutura do projeto
wk-ai-service/
â”œâ”€â”€ main.py              # AplicaÃ§Ã£o FastAPI
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ test_api.py         # Suite de testes
â”œâ”€â”€ README.md           # DocumentaÃ§Ã£o
â””â”€â”€ Dockerfile          # ContainerizaÃ§Ã£o
```

## ğŸ“¦ Docker

```bash
# Build
docker build -t wk-ai-service .

# Run
docker run -e GEMINI_API_KEY="AIzaSyD..." -p 8000:8000 wk-ai-service

# Docker Compose (jÃ¡ incluÃ­do no projeto)
docker-compose up wk-ai-service
```

## âš ï¸ LimitaÃ§Ãµes e ConsideraÃ§Ãµes

- **Rate Limiting**: Google Gemini tem limites de taxa (revise planos de uso)
- **Timeout**: Respostas podem levar 5-10 segundos
- **Cache**: Em-memory por padrÃ£o (considere Redis para produÃ§Ã£o)
- **Custo**: Gemini Ã© pago - monitore uso da API

## ğŸ› Debug

Logs detalhados estÃ£o habilitados. Para aumentar verbosidade:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ Changelog

### v1.0.0 (01/12/2026)
- Endpoint `/analyze` para anÃ¡lise de oportunidades
- Endpoint `/api/v1/chat` para chat com IA
- Cache em memÃ³ria
- Suporte completo ao Gemini Pro
- Logging detalhado
- CORS habilitado
- Test suite completa

## ğŸ¤ Suporte

Para questÃµes:
1. Verifique logs: `docker logs wk_ai_service`
2. Confirme GEMINI_API_KEY estÃ¡ configurada
3. Teste endpoints com `test_api.py`
4. Revise documentaÃ§Ã£o do Google Gemini

## ğŸ“„ LicenÃ§a

MIT
